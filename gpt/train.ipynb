{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2fcb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawData:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_name,\n",
    "        brain_name,\n",
    "        search_seed,\n",
    "        env_seed,\n",
    "        env_sdf,\n",
    "        goal,\n",
    "        robot_urdf,\n",
    "        score,\n",
    "    ):\n",
    "        self.env_name = env_name\n",
    "        self.brain_name = brain_name\n",
    "        self.search_seed = search_seed\n",
    "        self.env_seed = env_seed\n",
    "        self.env_sdf = env_sdf\n",
    "        self.goal = goal\n",
    "        self.robot_urdf = robot_urdf\n",
    "        self.score = score\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Data(\\n\"\n",
    "            f\"  env_name={self.env_name},\\n\"\n",
    "            f\"  brain_name={self.brain_name},\\n\"\n",
    "            f\"  search_seed={self.search_seed},\\n\"\n",
    "            f\"  env_seed={self.env_seed},\\n\"\n",
    "            f\"  env_sdf={self.env_sdf[0:50]},\\n\"\n",
    "            f\"  goal={self.goal},\\n\"\n",
    "            f\"  robot_urdf={self.robot_urdf[0:50]},\\n\"\n",
    "            f\"  score={self.score}\\n\"\n",
    "            \")\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a1292",
   "metadata": {},
   "source": [
    "# read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8045821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "data_dir = \"../final/data\"\n",
    "env_pattern = r\"terrain\\d+\"\n",
    "seed_list = [\"seed1234\", \"seed2345\"] \n",
    "brain_type = \"ea\"\n",
    "\n",
    "fitness_file = \"best_fitness.txt\"\n",
    "body_file = \"body.urdf\"\n",
    "brain_file = \"brain-100.nndf\"\n",
    "world_file = \"world.sdf\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75838b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path,\"r\") as f:\n",
    "        file_str = f.read()\n",
    "    return file_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec2eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dirs = []\n",
    "raw_data_list = []\n",
    "for e in os.listdir(data_dir):\n",
    "    env_path = os.path.join(data_dir,e)\n",
    "    match = re.match(r\"[A-Za-z]+(\\d+)\", e)\n",
    "    if match:\n",
    "        env_seed = int(match.group(1))\n",
    "    if os.path.isdir(env_path) and re.match(env_pattern,e):\n",
    "        for s in os.listdir(env_path):\n",
    "            seed_path = os.path.join(env_path,s)\n",
    "            if os.path.isdir(seed_path) and (s in seed_list):\n",
    "                for b in os.listdir(seed_path):\n",
    "                    if re.match(r\"body\\d+\",b):\n",
    "                        body_path = os.path.join(seed_path,b)\n",
    "                        raw_data_dirs.append(body_path)\n",
    "                        raw_data = RawData(\n",
    "                            env_name=e,\n",
    "                            brain_name=brain_type,\n",
    "                            search_seed=s,\n",
    "                            env_seed=env_seed,\n",
    "                            env_sdf=read_file(os.path.join(env_path, world_file)),\n",
    "                            goal=(100, 0, 0),\n",
    "                            robot_urdf=read_file(os.path.join(body_path, body_file)),\n",
    "                            score=(float(read_file(os.path.join(body_path, fitness_file))), 0 , 0),\n",
    "                        )\n",
    "                        raw_data_list.append(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b129b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data(\\n  env_name=terrain36199,\\n  brain_name=ea,\\n  search_seed=seed2345,\\n  env_seed=36199,\\n  env_sdf=<sdf>\\n    <model name=\"terrain0\">\\n        <pose>1.,\\n  goal=(100, 0, 0),\\n  robot_urdf=<robot name = \"robot\">\\n    <link name=\"link1\">\\n   ,\\n  score=(0.6332962657929321, 0, 0)\\n)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(raw_data_list[120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb8375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbd017b0",
   "metadata": {},
   "source": [
    "# save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25558bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "now = datetime.now()\n",
    "formatted_now = now.strftime(\"%y%m%d%H%M\")\n",
    "with open(f\"./data/rawdata_{formatted_now}.pkl\", \"wb\") as file:\n",
    "    pickle.dump(raw_data_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8dd86",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b6c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def read_data(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1829ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data= read_data(\"./data/rawdata_2304040613.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6268a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data(\\n  env_name=terrain36199,\\n  brain_name=ea,\\n  search_seed=seed1234,\\n  env_seed=36199,\\n  env_sdf=<sdf>\\n    <model name=\"terrain0\">\\n        <pose>1.,\\n  goal=(100, 0, 0),\\n  robot_urdf=<robot name = \"robot\">\\n    <link name=\"link1\">\\n   ,\\n  score=(0.24942633879748496, 0, 0)\\n)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(raw_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a242f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3b67514",
   "metadata": {},
   "source": [
    "# transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4990981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_urdf_to_sequence(urdf_string):\n",
    "    root = ET.fromstring(urdf_string)\n",
    "    # Extract the desired information\n",
    "    sequence = []\n",
    "    for element in root:\n",
    "        if element.tag == \"joint\":\n",
    "            parent_name = element.find(\"parent\").attrib[\"link\"]\n",
    "            node_name = element.find(\"child\").attrib[\"link\"]\n",
    "            joint_axis = element.find(\"axis\").attrib[\"xyz\"]\n",
    "            joint_type = element.attrib[\"type\"]\n",
    "            sequence.append((parent_name, node_name, joint_axis, joint_type))\n",
    "        elif element.tag == \"link\":\n",
    "            node_name = element.attrib[\"name\"]\n",
    "            # Extract the link type and size from the visual geometry\n",
    "            visual_geometry = element.find(\"visual/geometry\")\n",
    "            link_type = None\n",
    "            link_size = None\n",
    "            if visual_geometry is not None:\n",
    "                link_type = list(visual_geometry)[0].tag\n",
    "                if link_type == \"sphere\":\n",
    "                    radius = float(visual_geometry.find(\"sphere\").attrib[\"radius\"])\n",
    "                    link_size = (radius,radius,radius)\n",
    "                elif link_type == \"box\":\n",
    "                    size_str = visual_geometry.find(\"box\").attrib[\"size\"]\n",
    "                    link_size = tuple([float(s) for s in size_str.split()])\n",
    "                elif link_type == \"cylinder\":\n",
    "                    length = float(visual_geometry.find(\"cylinder\").attrib[\"length\"])\n",
    "                    radius = float(visual_geometry.find(\"cylinder\").attrib[\"radius\"])\n",
    "                    link_size = (radius,radius,length)\n",
    "\n",
    "            # Extract the sensor tag (assuming it's stored in the material name attribute)\n",
    "            sensor_color = element.find(\"visual/material\").attrib[\"name\"] if element.find(\"visual/material\") is not None else \"unknown\"\n",
    "            sensor_tag = True if \"sensored\" in sensor_color else False\n",
    "            sequence.append((node_name, link_type, link_size, sensor_tag))\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "044b9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sdf_to_gridmap(sdf_string, return_submap=False):\n",
    "    # Parse the SDF string\n",
    "    root = ET.fromstring(sdf_string)\n",
    "    # Create a grid map representation\n",
    "    grid_resolution = 1  # Adjust as needed\n",
    "    grid_size = [10, 10]  # Adjust as needed (this should be large enough to cover the entire space)\n",
    "    grid_map = np.zeros(grid_size, dtype=np.double) #grid_map = np.zeros(grid_size, dtype=np.uint8)\n",
    "    # Iterate over each model in the SDF file\n",
    "    for model in root.findall(\".//model\"):\n",
    "        # Extract box dimensions and pose\n",
    "        box_size = model.find(\".//box/size\").text.split()\n",
    "        box_size = [float(dim) for dim in box_size]\n",
    "        box_pose = model.find(\".//pose\").text.split()\n",
    "        box_position = [float(coord) for coord in box_pose[:3]]\n",
    "        box_rotation = [float(angle) for angle in box_pose[3:]]\n",
    "        # Fill in the grid cells corresponding to the box\n",
    "        offset = [int(coord / grid_resolution + size / 2) for (coord, size) in zip(box_position[0:2],grid_size)]\n",
    "        for i in range(offset[0], min(grid_size[0], offset[0] + int(box_size[0] / grid_resolution))):\n",
    "            for j in range(offset[1], min(grid_size[1], offset[1] + int(box_size[1] / grid_resolution))):\n",
    "                grid_map[i, j] = box_size[2] #1\n",
    "    if return_submap:\n",
    "        return grid_map[6:10,3:7]\n",
    "    else:\n",
    "        return grid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816494ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from src.utils.data_utils import PreprocessedData as PreprocessedData\n",
    "from src.utils.data_utils import read_data as read_data\n",
    "from src.utils.data_utils import save_data as save_data\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_sequence(self, data_file, save_dir=\"./data/preprocessed\", subset=None):   \n",
    "        raw_data = read_data(data_file)\n",
    "        processed_data = []\n",
    "        for single_data in raw_data[0:subset]:\n",
    "            sdf_sequence = convert_sdf_to_gridmap(single_data.env_sdf, return_submap=True)\n",
    "            goal_sequence = single_data.goal\n",
    "            urdf_sequence = convert_urdf_to_sequence(single_data.robot_urdf)\n",
    "            score_sequence = single_data.score\n",
    "            processed_data.append(PreprocessedData(sdf_sequence, goal_sequence, urdf_sequence, score_sequence))\n",
    "        \n",
    "        file_path = save_data(save_dir, processed_data)\n",
    "        return file_path\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08120890",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf = DataTransformer()\n",
    "prepcossed_file = data_tf.generate_sequence(\"./data/raw/rawdata_2304041648.pkl\",subset=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9986b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = read_data(prepcossed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c3c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1b36ac",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c2c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_utils import read_data as read_data\n",
    "preprocess_file = \"./data/preprocessed/data_2304041728.pkl\"\n",
    "preprocessed_data = read_data(preprocess_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754916b",
   "metadata": {},
   "source": [
    "### convert dataset to alpaca sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6b3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def construct_alpaca_data(preprocessed_data):\n",
    "    df = pd.DataFrame(columns=[\"instruction\", \"input_sequence\", \"output_sequence\"])\n",
    "    for single_data in preprocessed_data:\n",
    "        sdf_sequence, urdf_sequence, goal_sequence, score_sequence =single_data.round_sequence(2)\n",
    "        instruction = \"Based on the following gridmap and goal, give me the robot description and its score\"\n",
    "        input_sequence = f\"Grid map is {sdf_sequence}, the goal position is {goal_sequence}.\"\n",
    "        output_sequence = f\"Robot description is {urdf_sequence}, its final position is {score_sequence}\"\n",
    "        row_data = {\"instruction\": instruction, \"input_sequence\": input_sequence, \"output_sequence\":output_sequence }\n",
    "        df = df.append(row_data, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39bc382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpaca_df = construct_alpaca_data(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65242fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# now = datetime.now()\n",
    "# formatted_now = now.strftime(\"%y%m%d%H%M\")\n",
    "# file_path = f\"./data/alpacaed/data_{formatted_now}.json\"\n",
    "# alpaca_df.to_json(file_path, orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112c4bc",
   "metadata": {},
   "source": [
    "### step1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38430a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "tokenizer_path = \"./data/tutorial/tokenizer\"\n",
    "trainer_output_dir = \"./data/tutorial/model\"\n",
    "trainer_save_dir = \"./data/tutorial/finetuned_model\"\n",
    "preprocess_file = \"./data/preprocessed/data_2304041728.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5528546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_utils import read_data as read_data\n",
    "preprocessed_data = read_data(preprocess_file)\n",
    "dataset = [seq.round_str(2) for seq in preprocessed_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9252fa",
   "metadata": {},
   "source": [
    "### step2 hugging face fine tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ad044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n",
    "        return {key: tensor.squeeze(0) for key, tensor in encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c43d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def load_gpt_model(pretrained_model_name_or_path):\n",
    "    config = GPT2Config.from_pretrained(pretrained_model_name_or_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path, config=config)\n",
    "    return model\n",
    "\n",
    "def load_gpt_tokenizer(pretrained_model_name_or_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "    pad_token_exists = tokenizer.pad_token is not None\n",
    "    if not pad_token_exists:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d59ea8a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=1.7774594624837239, metrics={'train_runtime': 0.8334, 'train_samples_per_second': 35.995, 'train_steps_per_second': 10.799, 'total_flos': 1959690240000.0, 'train_loss': 1.7774594624837239, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained GPT model and tokenizer\n",
    "model_name_or_path = \"gpt2\"\n",
    "tokenizer = load_gpt_tokenizer(model_name_or_path)\n",
    "model = load_gpt_model(model_name_or_path)\n",
    "config = GPT2Config.from_pretrained(model_name_or_path)\n",
    "\n",
    "# initialize torch dataset\n",
    "train_dataset = CustomDataset(data=dataset[0:10], tokenizer=tokenizer, max_length=128)\n",
    "    \n",
    "# Fine-tune the model using your dataset\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=trainer_output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(trainer_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e549fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b301c63",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01883ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/model/gpt_model.py\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "def load_fine_tuned_gpt_model(model_dir):\n",
    "    config = GPT2Config.from_pretrained(model_dir)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_dir, config=config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b93c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_fine_tuned_gpt_model(trainer_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd2f387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_gpt_tokenizer(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc54bd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"grid map: [[0.11 0.18 0.18 0.02]\\n [0.15 0.05 0.01 0.17]\\n [0.19 0.17 0.14 0.04]\\n [0.05 0.04 0.18 0.13]], goal: (100, 0, 0), urdf: [('link1', 'sphere', (0.08, 0.08, 0.08), False), ('link2', 'cylinder', (0.1, 0.1, 0.22), True), ('link3', 'cylinder', (0.09, 0.09, 0.36), True), ('link1', 'link2', '0 1 0', 'spherical'), ('link2', 'link3', '0 1 0', 'revolute')], score: (0.25, 0, 0)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c66b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"grid map: [[0.11 0.18 0.18 0.02]\\n [0.15 0.05 0.01 0.17]\\n [0.19 0.17 0.14 0.04]\\n [0.05 0.04 0.18 0.13]], goal: (100, 0, 0),\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92a4cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "num_return_sequences = 1\n",
    "output_sequences = loaded_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc6e08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequences = []\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "    generated_sequences.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47a869aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grid map: [[0.11 0.18 0.18 0.02]\\n [0.15 0.05 0.01 0.17]\\n [0.19 0.17 0.14 0.04]\\n [0.05 0.04 0.18 0.13]], goal: (100, 0, 0), [(0, (0)], (1, 1), (2, 2), ((1 + (goal-1) * 2) - goal-0), 0);\\nThe last three values to examine are the (from 0 to 100) and the top five.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/model/inference.py\n",
    "\n",
    "def generate_text(prompt, model_dir, max_length=128, num_return_sequences=1):\n",
    "    tokenizer = load_gpt_tokenizer(model_dir)\n",
    "    model = load_fine_tuned_gpt_model(model_dir)\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "    )\n",
    "\n",
    "    generated_sequences = []\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        generated_sequences.append(text)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a2292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7f7a13",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8ae560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510ae0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.model.inference as inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39336a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data_preprocessing.data_transformer as data_transformer\n",
    "preprossed_file = \"./data/preprocessed/data_2304041728.pkl\"\n",
    "dataset = data_transformer.preprocess_data(preprossed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae4cd93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"grid map: [[0.11 0.18 0.18 0.02]\\n [0.15 0.05 0.01 0.17]\\n [0.19 0.17 0.14 0.04]\\n [0.05 0.04 0.18 0.13]], goal: (100, 0, 0), urdf: [('link1', 'sphere', (0.08, 0.08, 0.08), False), ('link2', 'cylinder', (0.1, 0.1, 0.22), True), ('link3', 'cylinder', (0.09, 0.09, 0.36), True), ('link1', 'link2', '0 1 0', 'spherical'), ('link2', 'link3', '0 1 0', 'revolute')], score: (0.25, 0, 0)\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4266bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51eeb9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"grid map: [[0.11 0.18 0.18 0.02]\\n [0.15 0.05 0.01 0.17]\\n [0.19 0.17 0.14 0.04]\\n [0.05 0.04 0.18 0.13]], goal: (100, 0, 0),\"\n",
    "# prompt = \"grid map: [[0.09 0.14 0.13 0.06]\\n [0.08 0.02 0.02 0.14]\\n [0.19 0.17 0.01 0.16]\\n [0.01 0.1  0.   0.13]], goal: (100, 0, 0),\"\n",
    "tokenizer_name_or_path = \"gpt2\"\n",
    "model_dir = \"./data/tutorial/finetuned_model\"\n",
    "model_dir = \"./data/tutorial/finetuned_model_2304050545\"\n",
    "model_dir = \"./data/tutorial/finetuned_model_2304051426\"\n",
    "max_length = 512 #256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf013e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "infered_sequence = inference.generate_text(prompt, tokenizer_name_or_path, model_dir, max_length, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "491982ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urdf: [('link1','sphere', (0,0), (1,1), True), ('link2', map ( 0 0 1, 1), False),('spherical')], score: (-0(0./0/0) errilla False)], map('cylinder', (-1.1,- 0., 0) True) ('cylcylinders', ('spike', ((0.,0.-0.\", 0.\",0.) True))],score: ('0.(0.. 0.) ile True, (-2 0 for 0…)], scored: ((1.. (- 0.. -0)) False, ('box', (\"0 0   0\", False); ('fixed', (−0\\.18, -18%, 0%, 'fixed')}, score:-0.: (2, 2, True)}], iles: ([0.08, −.08%, False)+ ('comment', [ 'box'] ( 'cylind 0.19, null), [spacing: 0.<spaces 0.</spacer>.\n",
      " (after 0 second, 'link3', False)} (link4', '[0 1 0', '(0', 0 part),'revolute'), ('links5',['cyl',('box1 0 FT', ['spoiler', ([('revure',0 '0  0 regulator', function '1'),''links2'), '[1 1', True', () 'linked3'), '(link51 ', \"spider', ({0.[0.]0], '2  '3 0 OFF', ())], result: '(1.13, ('1 False', (), ('revured'), (fixed), ['link6', '.1 CO 0 '',', '/spool', (!0.? 0.? 1, 'true'), ([link7', ', '4 01 ', False'), ($0._link8', \"\\cylid', (_link9', \"_box01 100 0′', \"'spark', (/0? 0? 1,- 1)), True'), (<link10', \"[0:14, \"+0″, \"-0 dB\", 'absolute'), ({\n"
     ]
    }
   ],
   "source": [
    "print(infered_sequence[0][123:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067bc9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f223bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_description = [('link1', 'box', (0.3, 0.3, 0.32), False), \n",
    "     ('link2', 'box', (0.2, 0.13, 0.32), True), \n",
    "     ('link3', 'box', (0.25, 0.39, 0.13), True), \n",
    "     ('link1', 'link2', '0 0 1', 'revolute'), \n",
    "     ('link2', 'link3', '0 1 0', 'revolute')] \n",
    "score = 0.38\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c84e448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
